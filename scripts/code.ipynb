{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import MultivariateNormal, Bernoulli\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_fig_folder = \"../report/figures/vae/\"\n",
    "cvae_fig_folder = \"../report/figures/cvae/\"\n",
    "ppca_fig_folder = \"../report/figures/ppca/\"\n",
    "os.makedirs(os.path.dirname(vae_fig_folder), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(cvae_fig_folder), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(ppca_fig_folder), exist_ok=True)\n",
    "os.makedirs(\"results/\", exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.MNIST(\n",
    "    root = '../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_set = datasets.MNIST(\n",
    "    root = '../data', train=False, download = True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vec = torch.reshape(train_set.data, (60000, 28*28)).type(torch.double)\n",
    "test_vec = torch.reshape(test_set.data, (10000, 28*28)).type(torch.double)\n",
    "N_test = len(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_comp = 6\n",
    "fig_h = 6\n",
    "indices = torch.randperm(len(test_vec))[:N_comp]\n",
    "test_sampled = test_vec[indices]\n",
    "test_img_sampled = test_sampled.reshape(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3600x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1, ax1 = plt.subplots(nrows=1, ncols=N_comp,\n",
    "                         figsize=(N_comp * fig_h / 1, fig_h))\n",
    "for j in range(N_comp):\n",
    "    ax1[j].axis(\"off\")\n",
    "    ax1[j].imshow(test_img_sampled[j])\n",
    "plt.savefig(ppca_fig_folder + \"real.pdf\", bbox_inches=\"tight\", facecolor=\"w\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda2 = False  # torch.cuda.is_available() can be used to check if a gpu is available - I just set it to False\n",
    "batch_size2 = 128\n",
    "log_interval2 = 10\n",
    "epochs2 = 4  # 10\n",
    "\n",
    "torch.manual_seed(1)  # args.seed\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda2 else \"cpu\")  # args.cuda\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda2 else {}  # args.cuda\n",
    "\n",
    "# Get train and test data\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "    batch_size=batch_size2, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "    batch_size=batch_size2, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc1a = nn.Linear(400, 100)\n",
    "        self.fc21 = nn.Linear(100, 2)  # Latent space of 2D\n",
    "        self.fc22 = nn.Linear(100, 2)  # Latent space of 2D\n",
    "        self.fc3 = nn.Linear(2, 100)  # Latent space of 2D\n",
    "        self.fc3a = nn.Linear(100, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc1a(h1))\n",
    "        return self.fc21(h2), self.fc22(h2)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        h4 = F.relu(self.fc3a(h3))\n",
    "        return torch.sigmoid(self.fc4(h4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        temp = self.decode(z).view(-1, 1, 28, 28)\n",
    "        return temp, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CVAE model\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Conv2d(1, 32, kernel_size=(\n",
    "            3, 3), stride=2, padding=1)  # nn.Linear(784, 400)\n",
    "        self.fc1a = nn.Conv2d(32, 64, kernel_size=(\n",
    "            3, 3), stride=2, padding=1)  # nn.Linear(400, 100)\n",
    "        W, H = 7, 7\n",
    "        self.fc21 = nn.Linear(64 * W * H, 2)  # Latent space of 2D\n",
    "        self.fc22 = nn.Linear(64 * W * H, 2)  # Latent space of 2D\n",
    "        self.fc3 = nn.Linear(2, 64 * W * H)  # Latent space of 2D\n",
    "        self.fc3a = nn.ConvTranspose2d(\n",
    "            64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.fc4 = nn.ConvTranspose2d(\n",
    "            32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc1a(h1))\n",
    "        h2 = h2.view(-1, 7 * 7 * 64)\n",
    "        return self.fc21(h2), self.fc22(h2)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        h3 = h3.view(-1, 64, 7, 7)\n",
    "        h4 = F.relu(self.fc3a(h3))\n",
    "        return torch.sigmoid(self.fc4(h4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_dims(in_dim, padding, kernel_size, dilation, stride):\n",
    "    return torch.floor((torch.tensor(in_dim + 2 * padding - dilation * (kernel_size -1) -1)/stride) + 1)\n",
    "\n",
    "def conv_t_dims(in_dim, padding, kernel_size, dilation, stride, output_padding):\n",
    "    return (in_dim -1) * stride - 2 * padding + dilation * (kernel_size -1) + output_padding + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, l = 1):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + l * KLD  # -ELBO if l = 1, if l = 0 then we have simply the log likelihood of x given z \n",
    "\n",
    "def train(model, optimizer, train_loader, epoch):\n",
    "    # so that everything has gradients and we can do backprop and so on...\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()  # \"reset\" gradients to 0 for text iteration\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, l = 1)\n",
    "        loss.backward()  # calc gradients\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()  # backpropagation\n",
    "    clear_output(wait=True)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(model, test_loader, mean = True, mse = False):\n",
    "    model.eval()\n",
    "    e_log_prob = 0\n",
    "    elbo = 0\n",
    "    mse_loss = 0\n",
    "    with torch.no_grad():  # no_grad turns of gradients...\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            e_log_prob += - loss_function(recon_batch, data, mu, logvar, l = 0).item()\n",
    "            elbo += - loss_function(recon_batch, data, mu, logvar, l = 1).item()\n",
    "            mse_loss += torch.nn.functional.mse_loss(recon_batch, data)\n",
    "    if mean :\n",
    "        e_log_prob /= len(test_loader.dataset)\n",
    "        mse_loss /= len(test_loader.dataset)\n",
    "        elbo /= len(test_loader.dataset)\n",
    "    print('====> Test set ELBO: {:.8f}'.format(elbo))\n",
    "    print('====> Test set expected log likelihood: {:.8f}'.format(e_log_prob))\n",
    "    print('====> Test set mse loss: {:.8f}'.format(mse_loss))\n",
    "    return e_log_prob, mse_loss, elbo\n",
    "\n",
    "\n",
    "def model_train(model, optimizer, train_loader, test_loader, epochs):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, optimizer, train_loader, epoch)\n",
    "        log_prob, mse_loss, elbo = test(model, test_loader, mean = True)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 2).to(device)  # 20 -> 2\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results/sample_' + str(epoch) + '.png')\n",
    "    return log_prob, mse_loss, elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = VAE().to(device)\n",
    "vae_optimizer = optim.Adam(vae_model.parameters(), lr=1e-3)\n",
    "# train some shit\n",
    "\n",
    "cvae_model = CVAE().to(device)\n",
    "cvae_optimizer = optim.Adam(cvae_model.parameters(), lr=1e-3)\n",
    "# train more shit\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 Average loss: 144.6963\n",
      "====> Test set ELBO: -145.12204819\n",
      "====> Test set expected log likelihood: -138.72355093\n",
      "====> Test set mse loss: 0.00030526\n"
     ]
    }
   ],
   "source": [
    "log_prob_vae, mse_loss_vae, elbo_vae = model_train(\n",
    "    vae_model, vae_optimizer, train_loader, test_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 6 Average loss: 160.7734\n",
      "====> Test set ELBO: -159.94936160\n",
      "====> Test set expected log likelihood: -154.21721299\n",
      "====> Test set mse loss: 0.00036178\n"
     ]
    }
   ],
   "source": [
    "log_prob_cvae, mse_loss_cvae, elbo_cvae = model_train(\n",
    "    cvae_model, cvae_optimizer, train_loader, test_loader, num_epochs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  basic VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test = list(test_loader)\n",
    "B = len(mnist_test)\n",
    "results = torch.zeros((128*B, 2))\n",
    "all_labels = torch.empty(0)\n",
    "l = 0\n",
    "for b in range(B):\n",
    "    labels = mnist_test[b][1]\n",
    "    all_labels = torch.cat((all_labels, labels))\n",
    "\n",
    "    images = mnist_test[b][0]\n",
    "    K = images.shape[0]\n",
    "    for k in range(K):\n",
    "        an_img = images[k, :, :, :]\n",
    "        an_img_flat = torch.flatten(an_img)\n",
    "        mean_img = vae_model.encode(an_img_flat)[0]\n",
    "        results[l] = mean_img\n",
    "        l += 1\n",
    "results = results[:l, :]\n",
    "all_labels = all_labels[:l]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_plot = results.detach().numpy()\n",
    "\n",
    "scatter_x = results_for_plot[:, 0]\n",
    "scatter_y = results_for_plot[:, 1]\n",
    "group = all_labels\n",
    "\n",
    "cdict = {0: 'black', 1: 'red', 2: 'blue', 3: 'green', 4: 'brown',\n",
    "         5: 'orange', 6: 'yellow', 7: 'magenta', 8: 'cyan', 9: 'purple'}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for g in np.unique(group):\n",
    "    ix = np.where(group == g)\n",
    "    ax.scatter(scatter_x[ix], scatter_y[ix], c=cdict[g], label=g, s=0.8)\n",
    "ax.legend(markerscale=10)\n",
    "plt.savefig(vae_fig_folder + \"clustering.pdf\",\n",
    "            bbox_inches=\"tight\", facecolor=\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 12\n",
    "x = torch.linspace(start=0, end=1, steps=M+2)\n",
    "x = x[:-1]\n",
    "x = x[1:]\n",
    "mesh_x, mesh_y = torch.meshgrid(x, x)\n",
    "gauss = torch.distributions.normal.Normal(loc=0, scale=1)\n",
    "gauss_x = gauss.icdf(mesh_x)  # .flip(0)\n",
    "gauss_y = gauss.icdf(mesh_y)\n",
    "fig, ax = plt.subplots(nrows=M, ncols=M, figsize=(8, 8), sharex = True, sharey = True)\n",
    "for i in range(M):\n",
    "    for j in range(M):\n",
    "        x_val = gauss_x[i,j].item()\n",
    "        y_val = gauss_y[i,j].item()\n",
    "        im = vae_model.decode(torch.Tensor([y_val, x_val]))\n",
    "        im_plt = im.reshape((28,28))\n",
    "        plot_me = im_plt.detach().numpy()\n",
    "        ax[i][j].imshow(plot_me)\n",
    "        ax[i][j].axis(\"off\")\n",
    "\n",
    "slim=0.6\n",
    "plt.tight_layout(pad=-slim, w_pad=-slim, h_pad=-slim)\n",
    "plt.savefig(vae_fig_folder + \"interpolation.pdf\",\n",
    "            bbox_inches=\"tight\", facecolor=\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_vae_sampled, _, _ = vae_model(test_sampled.type(torch.float))\n",
    "recons_vae_img_sampled = recons_vae_sampled.detach().reshape(-1, 28,28)\n",
    "fig2, ax2 = plt.subplots(nrows=1, ncols=N_comp,\n",
    "                         figsize=(N_comp * fig_h / 1, fig_h))\n",
    "for j in range(N_comp):\n",
    "    ax2[j].axis(\"off\")\n",
    "    ax2[j].imshow(recons_vae_img_sampled[j])\n",
    "plt.savefig(vae_fig_folder + \"mean.pdf\", bbox_inches=\"tight\", facecolor=\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_vae = Bernoulli(recons_vae_img_sampled)\n",
    "samples_vae = bernoulli_vae.sample()\n",
    "fig3, ax3 = plt.subplots(nrows=1, ncols=N_comp,\n",
    "                         figsize=(N_comp * fig_h / 1, fig_h))\n",
    "for j in range(N_comp):\n",
    "    ax3[j].axis(\"off\")\n",
    "    ax3[j].imshow(samples_vae[j])\n",
    "plt.savefig(vae_fig_folder + \"sample.pdf\", bbox_inches=\"tight\", facecolor=\"w\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test = list(test_loader)\n",
    "B = len(mnist_test)\n",
    "results = torch.zeros((128*B, 2))\n",
    "all_labels = torch.empty(0)\n",
    "l = 0\n",
    "for b in range(B):\n",
    "    labels = mnist_test[b][1]\n",
    "    all_labels = torch.cat((all_labels, labels))\n",
    "\n",
    "    images = mnist_test[b][0]\n",
    "    K = images.shape[0]\n",
    "    for k in range(K):\n",
    "        an_img = images[k, :, :, :]\n",
    "        #an_img_flat = torch.flatten(an_img)\n",
    "        mean_img = cvae_model.encode(an_img)[0]\n",
    "        results[l] = mean_img\n",
    "        l += 1\n",
    "results = results[:l, :]\n",
    "all_labels = all_labels[:l]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_plot = results.detach().numpy()\n",
    "\n",
    "scatter_x = results_for_plot[:, 0]\n",
    "scatter_y = results_for_plot[:, 1]\n",
    "group = all_labels\n",
    "\n",
    "cdict = {0: 'black', 1: 'red', 2: 'blue', 3: 'green', 4: 'brown',\n",
    "         5: 'orange', 6: 'yellow', 7: 'magenta', 8: 'cyan', 9: 'purple'}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for g in np.unique(group):\n",
    "    ix = np.where(group == g)\n",
    "    ax.scatter(scatter_x[ix], scatter_y[ix], c=cdict[g], label=g, s=0.8)\n",
    "ax.legend(markerscale=10)\n",
    "plt.savefig(cvae_fig_folder + \"clustering.pdf\",\n",
    "            bbox_inches=\"tight\", facecolor=\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 12\n",
    "x = torch.linspace(start=0, end=1, steps=M+2)\n",
    "x = x[:-1]\n",
    "x = x[1:]\n",
    "mesh_x, mesh_y = torch.meshgrid(x, x)\n",
    "gauss = torch.distributions.normal.Normal(loc=0, scale=1)\n",
    "gauss_x = gauss.icdf(mesh_x)  # .flip(0)\n",
    "gauss_y = gauss.icdf(mesh_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=M, ncols=M, figsize=(8, 8),\n",
    "                       sharex=True, sharey=True)\n",
    "for i in range(M):\n",
    "    for j in range(M):\n",
    "        x_val = gauss_x[i, j].item()\n",
    "        y_val = gauss_y[i, j].item()\n",
    "        im = cvae_model.decode(torch.Tensor([y_val, x_val]))\n",
    "        im_plt = im.reshape((28, 28))\n",
    "        plot_me = im_plt.detach().numpy()\n",
    "        ax[i][j].imshow(plot_me)\n",
    "        ax[i][j].axis(\"off\")\n",
    "\n",
    "slim = 0.6\n",
    "plt.tight_layout(pad=-slim, w_pad=-slim, h_pad=-slim)\n",
    "plt.savefig(cvae_fig_folder + \"interpolation.pdf\",\n",
    "            bbox_inches=\"tight\", facecolor=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_cvae_sampled, _, _ = cvae_model(test_sampled.type(torch.float).view(-1, 1,28,28))\n",
    "recons_cvae_img_sampled = recons_cvae_sampled.detach().reshape(-1, 28,28)\n",
    "fig2, ax2 = plt.subplots(nrows=1, ncols=N_comp,\n",
    "                         figsize=(N_comp * fig_h / 1, fig_h))\n",
    "for j in range(N_comp):\n",
    "    ax2[j].axis(\"off\")\n",
    "    ax2[j].imshow(recons_cvae_img_sampled[j])\n",
    "plt.savefig(cvae_fig_folder + \"mean.pdf\", bbox_inches=\"tight\", facecolor=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_cvae = Bernoulli(recons_cvae_img_sampled)\n",
    "samples_cvae = bernoulli_cvae.sample()\n",
    "fig3, ax3 = plt.subplots(nrows=1, ncols=N_comp,\n",
    "                         figsize=(N_comp * fig_h / 1, fig_h))\n",
    "for j in range(N_comp):\n",
    "    ax3[j].axis(\"off\")\n",
    "    ax3[j].imshow(samples_cvae[j])\n",
    "plt.savefig(cvae_fig_folder + \"sample.pdf\", bbox_inches=\"tight\", facecolor=\"w\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rather than estimating it directly using maximum likelihood, start by \n",
    "- estimating the data covariance matrix\n",
    "- calculate the eigenvectors and eigenvalues\n",
    "- to initialize ùëä (12.45)\n",
    "- Then use eq 12.46 to calculate ùúé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cov = torch.cov(data_vec.T)\n",
    "data_eig = torch.linalg.eig(data_cov)\n",
    "eig_vals = data_eig[0].real\n",
    "eig_vecs = data_eig[1].real\n",
    "#eig_vals = data_eig[0]\n",
    "#eig_vecs = data_eig[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W = U_M (L_M - \\sigma^2 I)^{1/2}$  \n",
    "U_M is DxM with columns being a subset of M eigenvectors of the covariance matrix.  \n",
    "L_M is MxM diagonal with corresponding eigenvalues.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = eig_vecs.shape[1]\n",
    "M = 2\n",
    "sig_sq = 1/(D-M)*torch.sum(eig_vals[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_M = eig_vecs[:,:2]\n",
    "L_M = torch.diag(eig_vals[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W = torch.matmul(U_M, (L_M-sig_sq*torch.eye(2))**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ùúá is given by the population mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.mean(data_vec, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now evaluate and plot the expected value of z given x for all your input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(z|x) = N(z | M^{-1}W^T(x-\\mu), \\sigma^{-2}M)$  \n",
    "$M = W^T W + \\sigma^2 I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_matrix = torch.matmul(W.T, W) + sig_sq * torch.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mu = test_vec - mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_matrix_inv = torch.inverse(M_matrix)\n",
    "temp = M_matrix_inv@W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_loc = temp@x_mu.T\n",
    "#z_loc = temp@x_mu.type(torch.complex128).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color the points by the digit labels. Does the Probabilistic PCA latent space separate the 10 digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best at 0 and 1 the rest are more overlapping. But some tendencies are seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_plot = z_loc.detach().numpy().T\n",
    "\n",
    "scatter_x = results_for_plot[:,0]\n",
    "scatter_y = results_for_plot[:,1]\n",
    "group = test_set.targets\n",
    "#group = trainset.targets\n",
    "cdict = {0: 'black', 1: 'red', 2: 'blue', 3: 'green', 4: 'brown', \n",
    "         5: 'orange', 6: 'yellow', 7: 'magenta', 8: 'cyan', 9: 'purple'}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for g in np.unique(group):\n",
    "    ix = np.where(group == g)\n",
    "    ax.scatter(scatter_x[ix], scatter_y[ix], c = cdict[g], label = g, s = 0.8)\n",
    "ax.legend(markerscale = 10)\n",
    "plt.savefig(ppca_fig_folder + \"clustering.pdf\", bbox_inches = \"tight\", facecolor = \"w\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute log-likelihood of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = torch.tensor(np.pi)\n",
    "C = W @ W.T + sig_sq * torch.eye(D)\n",
    "_, C_slogdet = torch.linalg.slogdet(C)\n",
    "inverse_C = torch.inverse(C)\n",
    "S = torch.cov(test_vec.T)\n",
    "#S = torch.cov(test_vec.type(torch.complex128).T)\n",
    "inner = D * torch.log(2*pi) + C_slogdet + torch.trace(inverse_C @ S)\n",
    "ppca_log_likelihood = (-N_test/2) * inner\n",
    "ppca_log_likelihood_mean = ppca_log_likelihood / N_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca_log_likelihood, ppca_log_likelihood_mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample latent variables z given test set samples x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z_cov = sig_sq * M_matrix_inv\n",
    "#z_cov = (1/sig_sq) * M_matrix\n",
    "z_dist = MultivariateNormal(z_loc.T, z_cov)\n",
    "z_samples = z_dist.sample()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project latent variables to mean parameters of likelihood functions $P(x | z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = z_samples @ W.T + mu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize mean parameters as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean_img = x_mean.reshape(-1, 28, 28)\n",
    "x_mean_img_sampled = x_mean_img[indices]\n",
    "fig2, ax2 = plt.subplots(nrows=1, ncols=N_comp, figsize=(N_comp * fig_h / 1, fig_h))\n",
    "for j in range(N_comp):\n",
    "    ax2[j].axis(\"off\")\n",
    "    ax2[j].imshow(x_mean_img_sampled[j])\n",
    "plt.savefig(ppca_fig_folder + \"mean.pdf\", bbox_inches = \"tight\", facecolor = \"w\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample reconstructions from the likelihood distributions\n",
    "And compute the log likelihood of the test dataset based on the likelihood distributions (rather than analytically as before). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cov = sig_sq * torch.eye(D).type(torch.double)\n",
    "log_prob_multivariate = 0\n",
    "x_multivariate_samples = torch.empty(N_test, D)\n",
    "for i in range(N_test):\n",
    "    x_multivariate = MultivariateNormal(x_mean[i], x_cov)\n",
    "    x_multivariate_samples[i] = x_multivariate.sample()\n",
    "    log_prob_multivariate += x_multivariate.log_prob(test_vec[i])\n",
    "log_prob_multivariate_mean = log_prob_multivariate / N_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_multivariate_mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sampled reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_multivariate_samples_img = x_multivariate_samples.reshape(-1, 28, 28)\n",
    "x_multivariate_samples_sampled = x_multivariate_samples_img[indices]\n",
    "fig3, ax3 = plt.subplots(nrows=1, ncols=N_comp, figsize=(N_comp * fig_h / 1, fig_h))\n",
    "for j in range(N_comp):\n",
    "    ax3[j].axis(\"off\")\n",
    "    ax3[j].imshow(x_multivariate_samples_sampled[j])\n",
    "plt.savefig(ppca_fig_folder + \"sample.pdf\", bbox_inches = \"tight\", facecolor = \"w\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore the latent space using a mesh grid and visualize the mean parameters of corresponding likelihood distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_steps = 12\n",
    "x = torch.linspace(start=0, end=1, steps=M_steps+2)\n",
    "x = x[:-1]\n",
    "x = x[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_x, mesh_y = torch.meshgrid(x, x)\n",
    "gauss = torch.distributions.normal.Normal(loc=0, scale=1)\n",
    "gauss_x = gauss.icdf(mesh_x)  # .flip(0)\n",
    "gauss_y = gauss.icdf(mesh_y)\n",
    "fig, ax = plt.subplots(nrows=M_steps, ncols=M_steps, figsize=(12, 12),\n",
    "                       sharex=True, sharey=True)\n",
    "for i in range(M_steps):\n",
    "    for j in range(M_steps):\n",
    "        x_val = gauss_x[i, j].item()\n",
    "        y_val = gauss_y[i, j].item()\n",
    "        z_val = torch.tensor([y_val, x_val], dtype = torch.double)\n",
    "        z_val_mean = z_val @ W.T + mu\n",
    "        im_plt = z_val_mean.type(torch.double).reshape((28, 28))\n",
    "        ax[i][j].imshow(im_plt)\n",
    "        ax[i][j].axis(\"off\")\n",
    "\n",
    "slim = 0.6\n",
    "plt.tight_layout(pad=-slim, w_pad=-slim, h_pad=-slim)\n",
    "plt.savefig(ppca_fig_folder + \"interpolation.pdf\",\n",
    "            bbox_inches=\"tight\", facecolor=\"w\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the mean squared error on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca_mse = torch.nn.functional.mse_loss(x_mean, test_vec, reduction = \"mean\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export metrics to latex file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "metrics =np.array([\n",
    "    #[log_prob_vae, mse_loss_vae],\n",
    "    [elbo_vae, mse_loss_vae],\n",
    "    #[log_prob_cvae, mse_loss_cvae],\n",
    "    [elbo_cvae, mse_loss_cvae],\n",
    "    [ppca_log_likelihood_mean, ppca_mse], \n",
    "    \n",
    "])\n",
    "df = pd.DataFrame(metrics)\n",
    "latex_headers = ['\\textbf{Log-Likelihood/ELBO}', '\\textbf{MSE}']\n",
    "df.columns = latex_headers\n",
    "df.index = [\"VAE\", \"CVAE\", \"PPCA\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_path = '../report/metrics.tex'\n",
    "os.makedirs(os.path.dirname(latex_path), exist_ok=True)\n",
    "latex_caption = 'Model performance metrics'\n",
    "latex_label = 'table:metrics'\n",
    "df.to_latex(latex_path, caption=latex_caption, \n",
    "            label=latex_label, escape=False, column_format='ccc', bold_rows=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c333c341eb371fc5e14c073d5b36648dab502d4cfc3bb985fa3f34b4687c0199"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
