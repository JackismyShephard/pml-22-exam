gaussian mixture model
    there exists a closed form solution for the likelihood given known parameters but it is a complex sum over log likelihood for each given latent variable z
        for each datapoint we sum its log prob given a specific z : logL(X | theta) = sum_{i=1}^{n} log( sum_{j=1}^{k} pi_j * N(x_i | mu_j, Sigma_j) )
            
            Where pi_j, mu_j and Sigma_j are the weight, mean, and covariance matrix of the j-th component and N(x_i | mu_j, Sigma_j) is the probability density function of a multivariate normal distribution with mean mu_j and covariance matrix Sigma_j evaluated at x_i.

            we can compute the log prob by using z to index into a list of means and covariance matrixes. this gives us m_k and cov_k we can then compute log N(x|mu_k, cov_k).
            
                we can either use torch.distributions.multivariatenormal.logprob or we can compute the log prob ourselves using the formula that was also used to compute the log prob for
                the ppca model (the formulas given on 574 of bishop book; the easiest is probably formula 12.44)
    
            we could also use torch.distributions.MixtureSameFamily.log_prob to compute the log prob
    because of this there does not exists a closed form solution for estimating the parameters for the maximum likelihood given a dataset
        so we have to use EM or use stochastic variational inference


todo
    check whether we should use  sigma^-2 * M or sigma^2 * inverse(M) for p(z|x) in ppca model 
    check why algorithm 1 in b.2 is returning the same minima each iteration
    compute log prob for gmm using manual method
    compute the mse for gmm
        for given x sample x from p(z|x), z defines a mean vector, which we reshape as image and compare with input x using mse

        QUESTION: how to sample from p(z|x), i.e. how is p(z|x) even defined
    
    For this, you can use convolutional layers followed by max-pooling layers to reduce the spatial resolution of the image and increase the number of feature maps.

    check why we cant use product of univariate normals for likelihood in ppca


    REASON Why we are getting positive log likelihood
        data is in 0 to 1 range instead of from 0 to 255



elbo
    The ELBO is defined as the expectation of the log-likelihood of the data under the approximate posterior, minus the KL divergence between the approximate posterior and the true posterior.

    The ELBO is a lower bound on the true log-likelihood of the data, hence the name "Evidence Lower Bound." The closer the approximate posterior is to the true posterior, the tighter the ELBO will be as an approximation of the log-likelihood. 

    In VAE, the reconstruction term in the ELBO loss is also a measure of how well the model is able to reconstruct the observed data, and it is also based on the assumption that the observed data is generated by a non-linear process with additive Gaussian noise. However, the reconstruction term in the ELBO loss is an approximation of the true log likelihood, and it is calculated using the approximate posterior of the latent variables.

    
    So in summary, the reconstruction term in the ELBO loss is a measure of how well the VAE is able to reconstruct the data, but it is not the same as the likelihood, which is intractable.

    The second term in the ELBO, KL(q(z|x) || p(z|x)), is the KL divergence between the approximate posterior and the true posterior, which acts as a regularization term that prevents the approximate posterior from being too different from the true posterior.

    If you want to compare a Probabilistic Principal Component Analysis (PPCA) model and a Variational Autoencoder (VAE), you would typically compare the log-likelihood of the PPCA model with the Evidence Lower Bound (ELBO) of the VAE.

    The ELBO provides a lower bound on the true log likelihood because the KL divergence term is always non-negative. By maximizing the ELBO during training, we are effectively maximizing a lower bound on the true log likelihood of the observed data.


vae
    The likelihood in a Variational Autoencoder (VAE) model is typically intractable, meaning it cannot be computed directly. However, an approximate likelihood can be obtained using the technique of variational inference

    QUESTION : can we assume that pixels are independent?
        It depends on the structure of the VAE and the data you are working with. In general, assuming independence among the pixels when computing the likelihood of an image can simplify the calculation but may not be an accurate reflection of the true dependence structure in the data.

    QUESTION:
        is the likelihood intractable or just finding the parameters that maximize the likelihood intractable?

            ANSWER: the likelihood for the vae is intractable for the same reason as for GMM, i.e. that we cannot easily derive a closed form expression for the parameters of the likelihood that maximize it. However, if we know the parameters of the likelihood function, then we can determine the likelihood of the data. 

            ANOTHER POINT: 
                the likelihood function we are using is from the family of independent (one for each pixel) multivariate bernoulli distributions
                    hence we are assuming that mnist data is black and white, which is not actually the case (another simplification)
                it would be more accurate to use a family of independent gaussian distributions, where std of each is 1 and the mean is determined by the decoder output
                    however, in this case it would be a little more complex to compute the log prob of the dataset (i.e. we would need to use torch.distributions.normal(decoder_output, std = 1).logprob(input_x)).

            FURTHER NOTE:
                This https://www.reddit.com/r/MachineLearning/comments/4ujr2s/comment/d5qj3m9/?utm_source=share&utm_medium=web2x suggests that binary cross entropy is used in VAE case mainly for better optimization behavior. Another reason it works well is that MNIST dataset roughly follows multivariate Bernoulli distribution - the pixel values are close to either zero or one and binarization does not change it much. For more in-depth explanation of this, see https://ruishu.io/2018/03/19/bernoulli-vae/ on real-valued observations.

                Note that this is exactly the application of Bernoulli observation VAEs to real-valued samples. So long as the real-valued samples can be interpreted as the Bernoulli distribution parameters, then this lower bound is valid. However, as noted above, this lower bound tends to be looser.


                If you have bounds on the range of values you expect, you can also normalize between 0-1, sigmoid the outputs and use binary crossentropy (you will have "targets" of 0.2, 0.8, 0.1, etc). This is related to the "Dark Knowledge" approximation work from Hinton et. al., but I think people used this trick before that for bernoulli-bernoulli RBMs on color image patches as well.

            CONCLUSION
                the elbo as we compute it is a lower bound on the actual log likelihood of the data, but a more lower bound than if it were computed using the correct formula for the likelihood of our data given current parameters (i.e. for example using a gaussian as described above and computing its log likelihood)




        ANSWER: 
            The reason for this is that a Bernoulli likelihood function is a simpler model, as it only has two possible values for each pixel (black or white), whereas a Gaussian likelihood function has a continuous range of possible values.

            Additionally, the Bernoulli likelihood function is useful when the goal is to learn a binary representation of the data, where each pixel is either black or white. This can be useful in certain applications such as denoising, where the goal is to remove noise from the images while preserving the underlying structure.

            Another reason why Bernoulli likelihood is used is that the VAE can use a Bernoulli likelihood even though the data is continuous, this is because the VAE's training process is based on optimization of an approximate likelihood, the ELBO, rather than the true likelihood. And with this approximation, it is possible that the VAE can still perform well even with a Bernoulli likelihood function.

            It is worth noting that while using a Bernoulli likelihood function is simpler, it may not be the best choice for all applications and the Gaussian likelihood function is more appropriate for the continuous range of pixel intensity values in the MNIST images.

    In VAE, the likelihood is intractable because it is based on a complex non-linear generative model that uses neural networks. 

    The likelihood in VAE is the probability of the observed data given the model parameters. It is intractable, since it involves integrating out the latent variables.



    However, an approximate likelihood can be obtained using the technique of variational inference. This involves introducing a set of latent variables, called the latent code, and introducing an auxiliary distribution, called the approximate posterior, which is used to approximate the true posterior distribution of the latent variables given the observed data. 
    
    Instead of computing the likelihood directly, VAE uses an approximation called the Evidence Lower BOund (ELBO) which is then optimized during the training process.
    
    The VAE model then uses the techniques of optimization and gradient descent to find the best values of the parameters of the approximate posterior, which will maximize the likelihood of the observed data.

    In VAEs, the KL divergence term in the loss function acts as a regularization term. It helps to keep the latent variables from collapsing to a single point, which would make the model over-confident in its predictions. The KL divergence term encourages the latent variables to explore different regions of the latent space, which in turn helps to improve the diversity of the generated samples. By minimizing the negative KL divergence term, we are actually maximizing the KL divergence between the approximate posterior and the prior. This leads to a more expressive and diverse latent space.
    The log likelihood term is the reconstruction loss which is the likelihood of the data given the latent variables. By maximizing the likelihood, we are trying to make the model produce realistic samples. Since the ELBO is the sum of the likelihood and the KL divergence, by minimizing it we are trying to balance the tradeoff between reconstruction accuracy and regularization.

ppca
    Probabilistic Principal Component Analysis (PPCA) is a probabilistic version of Principal Component Analysis (PCA), which is a linear dimensionality reduction technique. PPCA is a generative model that can be used to obtain a lower-dimensional representation of data by assuming that it was generated by a linear process, with additive Gaussian noise.


    Yes, that is correct. The likelihood in PPCA is more tractable than in VAE. In PPCA, the likelihood is based on a linear generative model with Gaussian noise, which makes it relatively simple to compute. The likelihood is a Gaussian distribution, and it's parameters (mean and covariance matrix) can be estimated using Expectation-Maximization algorithm.

    The log-likelihood value you're getting is extremely low, which suggests that the PPCA model is not a good fit for the MNIST data. There are a few reasons why this might be the case:

        The PPCA model assumes that the data is generated by a linear process, with additive Gaussian noise. However, the structure of the MNIST data is likely more complex than this.
        The PPCA model assumes that the noise is independent and identically distributed across the pixels. However, in the MNIST data, the noise across the pixels may not be independent, or the noise distribution may not be Gaussian.
        The PPCA model assumes that the data lies on a low-dimensional linear subspace. However, the MNIST data may not lie on a low-dimensional subspace.
        The PPCA model assumes that the number of principal components used to model the dependencies between the pixels is enough to capture the underlying structure of the data. However, using 2 principal components may not be enough to capture the underlying structure of the data.
    
    the log likelihood values are so small because the model is very complex and it is trying to fit a lot of parameters to the data. The high dimensionality of the data and the model make it difficult for the model to fit the data well. 

    in general, the log likelihood can take on small negative values even when the model is fitting the data well. This is because the likelihood function for high-dimensional data is often very small, and taking the logarithm of a small number results in a large negative value. So, It is not unusual to get small negative log-likelihood values

    The value of sigma^2 you provided is quite high. In general, a high value of sigma^2 indicates that the model has a high variance and is overfitting to the data. This could explain why the log likelihood values are so low.

    One way to check this is by looking at the eigenvalues of the data covariance matrix. If the eigenvalues are much larger than sigma^2, it suggests that the model is overfitting.

    he real part of M is a large value, which indicates that the covariance matrix of the data is also large. This in turn means that the variance of the data is large, and the data is spread out widely.

    The small log likelihood values you're seeing are likely due to the high dimensionality of the data and the limited number of principal components you're using. In high-dimensional spaces, it's common for likelihood values to be very small. Additionally, the high dimensionality of the data can make it difficult for PPCA to separate the different classes, which may be why the plot of the expected value of z given x doesn't appear to clearly separate the 10 digits.

    In PPCA, the projection from the input space to the latent space is done using a linear projection, so it is deterministic and there is no sampling involved. Therefore, when computing reconstruction loss, you would use the mean of the distribution over z, which is given by the linear projection of x onto the principal subspace defined by the eigenvectors of the covariance matrix.

ppca vs vae
    VAE, on the other hand, is a deep generative model that uses neural networks to learn non-linear representations of the data. Unlike PPCA, VAE is an unsupervised method that learns the underlying structure of the data by training on the data itself, rather than on labeled examples.

    Both PPCA and VAE can be used for dimensionality reduction, but VAE is a more general model that can handle complex data distributions and can be extended to other tasks such as image generation, anomaly detection, and representation learning. Additionally, VAE can also be used to generate new samples that are similar to the training data.

    So, in essence, both PPCA and VAE use a reconstruction term(log-likelihood) to measure how well the model is able to reconstruct the data, but the log-likelihood in PPCA is exact while the reconstruction term in VAE is an approximation.

    You are correct that PPCA and VAE are probabilistic versions of PCA and Autoencoder, respectively.

    PPCA is similar to PCA in that it uses a linear projection to reduce the dimensionality of the data. However, PPCA models the data as a probabilistic process, where the data is generated by a linear combination of a small number of latent variables and Gaussian noise.

    VAE, on the other hand, is similar to Autoencoder in that it uses a non-linear neural network to learn a compressed representation of the data. However, VAE models the data as a probabilistic process, where the data is generated by a latent variable z and a decoder network.

    Both PPCA and VAE are generative models that can be used to learn a compact representation of the data and to sample new data points from the learned model. However, VAE is a more powerful generative model than PPCA as it uses neural networks to model the complex non-linear relationships in the data.


MCMC
    This can be done by updating the kernel hyperparameters and the training data of the GP model using the posterior samples obtained from the previous iteration. You can use the mean or mode of the posterior samples as the new hyperparameters and training data.


VAE vs CVAE

     One way to improve the performance of your CVAE is to try different architectures (such as deeper or wider networks)

     it's possible that the convolutional VAE architecture you've implemented is not powerful enough to fully capture the features of the data.


     Some things you can try to improve the performance of the CVAE include:

        Increasing the number of filters in the convolutional layers to capture more complex features in the data
        Using deeper architectures with more layers
        Using data augmentation techniques like rotation, flipping or cropping
        Using dropout to prevent overfitting
    
    The architecture of the CVAE might not be well-suited for the task at hand. The convolutional layers in the CVAE might not be able to extract useful features from the input images, or the architecture might not be deep enough to effectively learn a useful latent representation of the data.

    The convolutional layers in the CVAE may not be able to extract useful features from the data, as the images in the dataset may be too simple and don't require convolutional layers to extract features.

    Additionally, convolutional layers can add additional complexity to the model, making it harder to train

    The second thing is that, the size of the convolutional kernels is too small, using kernel of size (3,3) and stride 2 will reduce the image size too much before reaching the fully connected layers which results in loss of information.

    The third thing is that, The number of feature maps used in the convolutional layers are low, increasing the number of feature maps can help the model learn more complex features in the input images.


    it is possible that the regular VAE is performing better on the MNIST dataset because it is a relatively simple dataset and the added complexity of the convolutional layers is not providing enough additional information to improve the ELBO over the simpler model. Also, it's possible that the convolutional layers are not capturing the spatial information of the images in a useful way. Since the MNIST digits are centered and of similar size, the convolutional layers might not be necessary. It's also possible that the added complexity of the convolutional layers is making it harder to find good parameters and regularization settings. 


gmm vs ppca