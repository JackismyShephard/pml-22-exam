\paragraph{Essential properties and advantages/disadvantages}
The first model which we implement is the probabilistic PCA model introduced in Chapter 12.2.1 of \citep{bishop2006pattern}. The reason this model is attractively is primarily due to the fact that its likelihood is tractable. In particular, given a dataset $\bm{X} = \lrc{\bm{x}_n}$ of $N$ samples $\bm{x}_n$ with $D$ dimensions each, the log likelihood of the PPCA model is given by $\ln p(\bm{X}|\bm{W}, \bm{u}, \sigma^2) = -\frac{N}{2}\lr{D \ln(2\pi) + \ln \lra{\bm{C}} + \text{Tr}\lr{\bm{C}^{-1}\bm{S}}}$, where $\bm{C} = \bm{W}\bm{W}^T + \sigma^2 \bm{I}$ and $\bm{S}$ is the sample covariance matrix of $\bm{X}$. Moreover, there exists a closed form-solution for the parameters $\bm{W}, \bm{u}, \sigma^2$, which maximize this log-likelihood. 

The existence of a closed-form expression for the likelihood function is useful, as it allows for direct comparison with other density model. The fact that its the maximizing parameters also have a closed-form means we can easily optimize the PPCA model. At the same time, the PPCA model also allows for numerical optimization using the Expectation-Maximization (EM) algorithm, which can be more efficient in higher dimensions and when dealing with missing data. From a practical point of view, the PPCA model is useful, as it can be used both for dimensionality-reduction (like the regular PCA model), but also for random sampling due to its probabilistic nature. Finally, a major advantage of the PPCA model is that it allows capturing the most important covariance in its included principal components, while averaging the variance for all the remaining left-out principal components in its parameter $\sigma^2$. 

Perhaps the most obvious disadvantage of the PPCA model is that it is based on a linear-gaussian framework and hence, unlike the VAE, may not be able to capture complex non-linear structure in the data it is trained on. On another note, it is worth pointing out that the maximum-likelihood solution to the PPCA model only determines the latent space up to an arbitrary rotation $\bm{R}$. We shall for simplicity set $\bm{R} = \bm{I}$. Moreover, we shall use $M=2$ principal components, so that we may easily compare the latent space of the PPCA model with that of the previously introduced VAE models.

\paragraph{Model performance}
We assess whether the PPCA model is better or worse than the original VAE by using the same quantitative measures and qualitative experiments used to compare the CVAE with the VAE\@. For the MSE loss we compare images $\bm{x}_i$ from the MNIST test set with corresponding mean parameter values $\bm{\eta}_i$ obtained by first mapping each $\bm{x}_i$ to a posterior distribution $p(\bm{z}_i|\bm{x}_i) = \mathcal{N}\lr{\bm{z}| \text{proj}\lr{\bm{x}_i}, \sigma^2 \bm{M}^{-1}}$,where  $\text{proj}(\bm{x}_i) = \bm{M}^{-1} \bm{W}^T\lr{\bm{x}_i - \bm{\mu}}$ and $\bm{M} = \bm{W}^{T}\bm{W} + \sigma^2 \bm{I}$ ,then sampling a $\bm{z}_i$ from this distribution, and finally computing $\bm{\eta}(\bm{z}_i) = \bm{W}\bm{z}_i + \bm{\mu}_i$. This same procedure is used for the reconstruction experiment, though in this case we compare select $\bm{x}_i$ and corrresponding $\bm{\eta}_i$ visually. For the clustering experiment, we reuse the projection  $\text{proj}\lr{\bm{x}_i}$ to map each sample $\bm{x}_i$ to its corresponding mean parameter in latent space. Symmetrically, for the exploration of latent space we use reuse the back-projection $\bm{\eta}(\bm{z}_i)$ for mapping the deterministically sampled latent variables $\bm{z}_i$ to corresponding mean parameters that are subsequently represented as images.

As can be seen in \cref{table:metrics}, the sample-wise log likelihood of the PPCA model is abysmally low compared to that of the original VAE\@. One explanation why the metric is so low may be that the MNIST is rather large. Hence, even with a modest average probability $p(\bm{x}_i | \bm{W}, \bm{u}, \sigma^2)$ of each sample $\bm{x}_i$ in the MNIST test-set, we still may end up with a very small total likelihood $p(\bm{X}|\bm{W}, \bm{u}, \sigma^2 ) = \prod_{i}^{N}p(\bm{x}_i | \bm{W}, \bm{u}, \sigma^2)$. On the other hand, considering the fact that the MSE of the PPCA is also abysmally low compared to that of the original VAE, it seems likely that the PPCA model has neither learned to model the underlying distribution of the MNIST dataset nor to reconstruct it samples accurately. This conjecture is supported by the qualitative experiments. Based on \cref{fig:ppca:clustering}, one may infer that the PPCA model has not learned to separate samples in latent space based on labels very well. In particular, it seems only capable of separating samples with labels that are either 1 or 0. The exploration of latent space shown in \cref{fig:ppca:interpolation} seem to further solidity the idea that the PPCA model has not learned a proper representation of other digits than 1, 0 and perhaps 9. Similarly, the reconstructions in \cref{fig:ppca:mean} are very bad, with only 0 and 1 being somewhat accurately reconstructed.

Why the PPCA model performs so much worse than the original VAE is most likely due to its aforementioned linear components, which are not capable of capturing the non-linear nature of the MNIST dataset. On the other hand, because the VAE utilizes components such as ReLU activation functions, it can more easily model the complexities of the MNIST dataset. The PPCA model might also perform poorly due to the fact that the samples from the MNIST dataset are very high dimensional and may not necessarily lie on a low-dimensional manifold, which a latent space based on $2$ principal components assumes. In general, a very low likelihood may be expected when dealing with data that is as high-dimensional as the samples from the MNIST dataset. 