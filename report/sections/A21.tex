\paragraph{Essential properties and advantages/disadvantages}
The first model which we implement is the probabilistic PCA model introduced in Chapter 12.2.1 of \citep{bishop2006pattern}. The reason this model is attractively is primarily due to the fact that its likelihood is tractable. In particular, given a dataset $\bm{X} = \lrc{\bm{x}_n}$ of $N$ samples $\bm{x}_n$ with $D$ dimensions each, the log likelihood of the PPCA model is given by $\ln p(\bm{X}|\bm{W}, \bm{u}, \sigma^2) = -\frac{N}{2}\lr{D \ln(2\pi) + \ln \lra{\bm{C}} + \text{Tr}\lr{\bm{C}^{-1}\bm{S}}}$, where $\bm{C} = \bm{W}\bm{W}^T + \sigma^2 \bm{I}$ and $\bm{S}$ is the sample covariance matrix of $\bm{X}$. Moreover, there exists a closed form-solution for the parameters $\bm{W}, \bm{u}, \sigma^2$, which maximize this log-likelihood. 

The existence of a closed-form expression for the likelihood function is useful, as it allows for direct comparison with other density model. The fact that its the maximizing parameters also have a closed-form means we can easily optimize the PPCA model. At the same time, the PPCA model also allows for numerical optimization using the Expectation-Maximization (EM) algorithm, which can be more efficient in higher dimensions and when dealing with missing data. From a practical point of view, the PPCA model is useful, as it can be used both for dimensionality-reduction (like the regular PCA model), but also for random sampling due to its probabilistic nature. Finally, a major advantage of the PPCA model is that it allows capturing the most important covariance in its included principal components, while averaging the variance for all the remaining left-out principal components in its parameter $\sigma^2$. 

Perhaps the most obvious disadvantage of the PPCA model is that it is based on a linear-gaussian framework and hence, unlike the VAE, may not be able to capture complex non-linear structure in the data it is trained on. On another note, it is worth pointing out that the maximum-likelihood solution to the PPCA model only determines the latent space up to an arbitrary rotation $\bm{R}$. We shall for simplicity set $\bm{R} = \bm{I}$. Moreover, we shall use $M=2$ principal components, so that we may easily compare the latent space of the PPCA model with that of the previously introduced VAE models.

\paragraph{Model performance}
We assess whether the PPCA model is better or worse than the original VAE by using the same quantitative measures and qualitative experiments used to compare the CVAE with the VAE. For the MSE loss we compare images $\bm{x}_i$ from the MNIST test set with corresponding mean parameter values $\bm{\eta}_i$ obtained by first mapping each $\bm{x}_i$ to a posterior distribution $p(\bm{z}_i|\bm{x}_i) = \mathcal{N}\lr{\bm{z}| \text{proj}\lr{\bm{x}_i}, \sigma^2 \bm{M}^{-1}}$,where  $\text{proj}(\bm{x}_i) = \bm{M}^{-1} \bm{W}^T\lr{\bm{x}_i - \bm{\mu}}$ and $\bm{M} = \bm{W}^{T}\bm{W} + \sigma^2 \bm{I}$ ,then sampling a $\bm{z}_i$ from this distribution, and finally computing $\bm{\eta}(\bm{z}_i) = \bm{W}\bm{z}_i + \bm{\mu}_i$. This same procedure is used for the reconstruction experiment, though in this case we compare select $\bm{x}_i$ and corrresponding $\bm{\eta}_i$ visually. For the clustering experiment, we reuse the projection  $\text{proj}\lr{\bm{x}_i}$ to map each sample $\bm{x}_i$ to its corresponding mean parameter in latent space. Symmetrically, for the exploration of latent space we use reuse the back-projection $\bm{\eta}(\bm{z}_i)$ for mapping the deterministically sampled latent variables $\bm{z}_i$ to corresponding mean parameters that are subsequently represented as images.