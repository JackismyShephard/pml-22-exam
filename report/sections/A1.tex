
\paragraph{Architecture of the convolutional VAE}
Our implementation of the convolutional VAE is similar in architecture to the original VAE\@. The primary difference between the two models is that the two linear layers in the encoder and decoder part of the original VAE have been replaced by respectively two convolutional layers and two transposed convolutional layers in the convolutional VAE\@. The two convolutional layers in the encoder part of the convolutional VAE both use kernels of size $3\times 3$, $1\times 1$ padding and a stride of $2\times2$. The first convolutional layer has $1$ input channel and $16$ output channels, while the second convolutional layers has $16$ input channels and $32$ output channels. \iffalse As such, each convolutional layer decreases the spatial dimensions of their input by a factor of $2$, and the second convolutional layer additionally increases the feature dimension of its input by a factor $2$ to offset the loss in spatial dimensions. \fi The transposed convolutional layers in the decoder part of the convolutional VAE complement the aforementioned convolutional layers. Both transposed convolutional layers use kernels of size $3\times3$, $1\times 1$ padding and output padding as well as a stride of $2\times 2$. The first transposed convolutional layer has $32$ input channels and $16$ output channels, while the second transposed convolutional layers has $16$ input channels and $1$ out channel. \iffalse As such, each transposed convolutional layers "undoes" the spatial reduction produced by a corresponding prior convolutional layer by upsampling the spatial dimensions of its input with a factor of $2$. \fi Conceptually, the encoder part of the convolutional VAE condenses its input images $\bm{x}$ of size $D = N \times N$ into a compact but feature rich representation $\bm{z}$ of size $C = 2$, while the corresponding decoder "unfolds" this compact representation in reverse order, ultimately producing output images $\bm{y}$ of the same size as $\bm{x}$ . 
\paragraph{Parameter estimation}
In order to optimize the convolutional VAE model we estimate the parameters $\bm{\phi}$ and $\bm{\theta}$ that maximize the ELBO, which is a lower bound on the log likelihood of the data $\bm{x}$ and is defined as $\mathcal{L}(\bm{\theta}, \bm{\phi}, \bm{x}) = \E_{q_{\bm{\phi}}(\bm{z}|\bm{x})}{[\ln{p_{\bm{\theta}}(\bm{x} |\bm{z} )}]} - \KL (q_{\bm{\phi}}(\bm{z}|\bm{x}) || p_{\bm{\theta}}(\bm{z}) )$. This is the same criterion used to optimize the original VAE\@. In particular, both models use the reparameterization trick to sample latent variables $\bm{z}$ from a factorized gaussian approximate posterior $q_{\bm{\phi}}(\bm{z}|\bm{x}) = \mathcal{N}(\bm{z}; \bm{u}, \text{diag}(\bm{\sigma}^2))$, so the Kullback-Leibler divergence, which acts as a regularization term in the ELBO, simplifies to $\frac{1}{2}\sum_{d=1}^{D}(\sigma_d^2 + \mu_d^2 - 1 - \ln(\sigma_d^2))$, assuming that the prior over $\bm{z}$ is also a diagonal gaussian $p_{\theta}(\bm{z}) = \mathcal{N}(\bm{z};\bm{0}, \bm{I})$. Here $\ln{\bm{\sigma}^2}$ and $\bm{\mu}$ are the outputs of the encoder when applied to $\bm{x}$. In order to compute the first term in the ELBO, which is the expected likelihood of $\bm{x}$ given $\bm{z}$, we utilize the output $\bm{y}$ of the decoder. We fed $\bm{y}$ fed through a sigmoid activation layer, which produces the mean parameters $\bm{p}$ of a factorized multivariate Bernoulli distribution $\prod_j^{D} \text{Bernoulli}(x_j; p_j)$ acting as the likelihood $p_{\bm{\theta}}(\bm{x} |\bm{z} )$ of the input $\bm{x}$ given latent variable $\bm{z}$. \iffalse As such, one may regard both VAE models as making the simplifying assumption that the MNIST data is binary, which is not technically the case. The simplifying assumption is most likely warranted given that the vast majority of pixels in the MNIST dataset are in fact either $0$ or $1$. Given this simplification , \fi We then approximate  the expected log likelihood by computing the negative of the binary cross entropy of $\bm{p}$ and $\bm{x}$, where $\bm{x}$ is treated as a set of probability values. Given the simplified ELBO, we optimize the parameters of both models using a variant of gradient descent, namely the Adam algorithm with a learning rate of $0.001$ and a batch size of $128$. In order to ensure convergence we run the Adam algorithm for a total of $25$ epochs for each model. We train both models on the complete MNIST training set.